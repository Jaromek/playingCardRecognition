{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4532039,"sourceType":"datasetVersion","datasetId":2579480},{"sourceId":10566397,"sourceType":"datasetVersion","datasetId":6538421},{"sourceId":10615646,"sourceType":"datasetVersion","datasetId":6560922}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\nfrom torchvision import datasets, transforms\nfrom PIL import Image\nfrom collections import Counter\nfrom torchvision.datasets import ImageFolder\nimport pandas as pd\nfrom tqdm import tqdm\nimport shutil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:10:10.363565Z","iopub.execute_input":"2025-02-10T12:10:10.363863Z","iopub.status.idle":"2025-02-10T12:10:17.73956Z","shell.execute_reply.started":"2025-02-10T12:10:10.363812Z","shell.execute_reply":"2025-02-10T12:10:17.738941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Add Augmented Data for Training","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms.functional import rotate\n\n# Define paths\ntrain_dir = \"/kaggle/input/cards-image-datasetclassification/train\"\n\n# Define augmentation transformations\naugmentations = {\n    \"rot_30\": lambda img: rotate(img, 30, expand=True),\n    \"rot_90\": lambda img: rotate(img, 90, expand=True)\n}\n\n# Output directory for augmented images\naugmented_train_dir = \"/kaggle/working/augmented_train\"\nif os.path.exists(augmented_train_dir):\n    shutil.rmtree(augmented_train_dir)\nshutil.copytree(train_dir, augmented_train_dir)\n\n# Loop through each class directory and apply augmentations\nfor card_class in tqdm(os.listdir(train_dir), desc=\"Processing classes\"):\n    class_path = os.path.join(train_dir, card_class)\n    augmented_class_path = os.path.join(augmented_train_dir, card_class)\n    \n    if not os.path.isdir(class_path):\n        continue  # Skip non-directory files\n    \n    for img_name in os.listdir(class_path):\n        img_path = os.path.join(class_path, img_name)\n        \n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {img_name}: {e}\")\n            continue\n        \n        # Apply augmentations and save them\n        for aug_name, aug_func in augmentations.items():\n            augmented_img = aug_func(image)\n            \n            augmented_img_name = f\"{os.path.splitext(img_name)[0]}_{aug_name}.jpg\"\n            augmented_img_path = os.path.join(augmented_class_path, augmented_img_name)\n            augmented_img.save(augmented_img_path, \"JPEG\")\n            \nprint(\"Data augmentation completed. Augmented images are stored in:\", augmented_train_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:10:17.740296Z","iopub.execute_input":"2025-02-10T12:10:17.74064Z","iopub.status.idle":"2025-02-10T12:11:55.530586Z","shell.execute_reply.started":"2025-02-10T12:10:17.740602Z","shell.execute_reply":"2025-02-10T12:11:55.529789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_dir = \"/kaggle/input/cards-image-datasetclassification/valid\"\ntest_dir = \"/kaggle/input/cards-image-datasetclassification/test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:11:55.531483Z","iopub.execute_input":"2025-02-10T12:11:55.531808Z","iopub.status.idle":"2025-02-10T12:11:55.535192Z","shell.execute_reply.started":"2025-02-10T12:11:55.531775Z","shell.execute_reply":"2025-02-10T12:11:55.53453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nimage_size = (224, 224)\nnum_classes = 53\nlearning_rate = 0.001\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:11:55.536046Z","iopub.execute_input":"2025-02-10T12:11:55.536314Z","iopub.status.idle":"2025-02-10T12:11:55.628718Z","shell.execute_reply.started":"2025-02-10T12:11:55.536294Z","shell.execute_reply":"2025-02-10T12:11:55.627849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_transforms = {\n    \"train\": transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.ToTensor()\n    ]),\n    \"valid\": transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.ToTensor()\n    ]),\n    \"test\": transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.ToTensor()\n    ]),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:11:55.632317Z","iopub.execute_input":"2025-02-10T12:11:55.632522Z","iopub.status.idle":"2025-02-10T12:11:55.641883Z","shell.execute_reply.started":"2025-02-10T12:11:55.632504Z","shell.execute_reply":"2025-02-10T12:11:55.641086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = ImageFolder(augmented_train_dir, transform=data_transforms[\"train\"])\nvalid_dataset = ImageFolder(valid_dir, transform=data_transforms[\"valid\"])\ntest_dataset = ImageFolder(test_dir, transform=data_transforms[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:11:55.642775Z","iopub.execute_input":"2025-02-10T12:11:55.643115Z","iopub.status.idle":"2025-02-10T12:11:56.914323Z","shell.execute_reply.started":"2025-02-10T12:11:55.643084Z","shell.execute_reply":"2025-02-10T12:11:56.913405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:11:56.915222Z","iopub.execute_input":"2025-02-10T12:11:56.915549Z","iopub.status.idle":"2025-02-10T12:11:56.919833Z","shell.execute_reply.started":"2025-02-10T12:11:56.915517Z","shell.execute_reply":"2025-02-10T12:11:56.919035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, expansion=2):\n        super().__init__()\n        expanded_channels = in_channels * expansion\n        self.conv1 = nn.Conv2d(in_channels, expanded_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(expanded_channels)\n        self.conv2 = nn.Conv2d(expanded_channels, in_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n    \n    def forward(self, x):\n        residual = x\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        x += residual\n        return F.relu(x)\n\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Initial Conv Block\n        self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1)  # 112x112\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        # Stage 1: 112x112\n        self.stage1 = nn.Sequential(\n            ResidualBlock(64),\n            ResidualBlock(64),\n            nn.MaxPool2d(2, 2)  # 56x56\n        )\n        \n        # Stage 2: 56x56 → 28x28\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.stage2 = nn.Sequential(\n            ResidualBlock(128),\n            ResidualBlock(128),\n            nn.MaxPool2d(2, 2)  # 28x28\n        )\n        \n        # Stage 3: 28x28 → 14x14\n        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.stage3 = nn.Sequential(\n            ResidualBlock(256),\n            ResidualBlock(256),\n            nn.MaxPool2d(2, 2)  # 14x14\n        )\n        \n        # Head\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))  # 112x112\n        x = self.stage1(x)  # 56x56\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.stage2(x)  # 28x28\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.stage3(x)  # 14x14\n        x = self.gap(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:31.083242Z","iopub.execute_input":"2025-02-10T12:12:31.083558Z","iopub.status.idle":"2025-02-10T12:12:31.092794Z","shell.execute_reply.started":"2025-02-10T12:12:31.083532Z","shell.execute_reply":"2025-02-10T12:12:31.091936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CNNModel(num_classes = num_classes).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:31.552229Z","iopub.execute_input":"2025-02-10T12:12:31.552494Z","iopub.status.idle":"2025-02-10T12:12:31.881393Z","shell.execute_reply.started":"2025-02-10T12:12:31.552474Z","shell.execute_reply":"2025-02-10T12:12:31.880741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(y_pred,y_true):\n    y_pred = F.softmax(y_pred,dim = 1)\n    top_p,top_class = y_pred.topk(1,dim = 1)\n    equals = top_class == y_true.view(*top_class.shape)\n    return torch.mean(equals.type(torch.FloatTensor))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:34.083441Z","iopub.execute_input":"2025-02-10T12:12:34.08372Z","iopub.status.idle":"2025-02-10T12:12:34.088182Z","shell.execute_reply.started":"2025-02-10T12:12:34.0837Z","shell.execute_reply":"2025-02-10T12:12:34.087233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ModelTrainer():\n    def __init__(self, criterion=None, optimizer=None, schedular=None, patience=6, l1_lambda=1e-5):\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.schedular = schedular\n        self.patience = patience  # Early stopping patience\n        self.early_stop = False\n        self.wait = 0  # Counter for patience\n        self.best_valid_loss = np.Inf  # Best validation loss\n        self.best_val_acc = 0\n        self.best_train_acc = 0\n        self.l1_lambda = l1_lambda  # L1 regularization strength\n\n    def compute_l1_penalty(self, model):\n        l1_penalty = 0.0\n        for param in model.parameters():\n            if param.requires_grad:\n                l1_penalty += torch.sum(torch.abs(param))\n        return l1_penalty\n\n    def train_batch_loop(self, model, trainloader):\n        train_loss = 0.0\n        train_acc = 0.0\n\n        for images, labels in tqdm(trainloader):\n            # Move the data to device (CPU/GPU)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = self.criterion(outputs, labels)\n\n            # Add L1 penalty to loss\n            l1_penalty = self.compute_l1_penalty(model)\n            loss += self.l1_lambda * l1_penalty\n\n            # Backward pass and optimization\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            train_loss += loss.item()\n            train_acc += accuracy(outputs, labels)\n\n        return train_loss / len(trainloader), train_acc / len(trainloader)\n\n    def valid_batch_loop(self, model, validloader):\n        valid_loss = 0.0\n        valid_acc = 0.0\n\n        for images, labels in tqdm(validloader):\n            # Move the data to device (CPU/GPU)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                outputs = model(images)\n                loss = self.criterion(outputs, labels)\n\n            valid_loss += loss.item()\n            valid_acc += accuracy(outputs, labels)\n\n        return valid_loss / len(validloader), valid_acc / len(validloader)\n\n    def fit(self, model, trainloader, validloader, epochs):\n        for i in range(epochs):\n            if self.early_stop:\n                print(\"Early stopping triggered. Exiting training loop.\")\n                break\n\n            # Training phase\n            model.train()\n            avg_train_loss, avg_train_acc = self.train_batch_loop(model, trainloader)\n\n            # Validation phase\n            model.eval()\n            avg_valid_loss, avg_valid_acc = self.valid_batch_loop(model, validloader)\n\n            # Check for improvement in validation loss\n            if avg_valid_loss < self.best_valid_loss: \n                print(f\"Validation loss decreased ({self.best_valid_loss:.6f} --> {avg_valid_loss:.6f}). Saving model...\")\n                torch.save(model.state_dict(), f'cnn_v10_res22k_{avg_train_acc:.2f}_{avg_valid_acc:.2f}.pt')\n                self.best_valid_loss = avg_valid_loss\n                self.wait = 0  # Reset patience counter\n            elif avg_train_acc > self.best_train_acc and avg_valid_acc > self.best_val_acc: \n                print(f\"Training accuracy increased ({self.best_train_acc:.6f} --> {avg_train_acc:.6f}).\")\n                print(f\"Validation accuracy increased ({self.best_val_acc:.6f} --> {avg_valid_acc:.6f}). Saving model...\")\n                torch.save(model.state_dict(), f'cnn_v10_res22k_{avg_train_acc:.2f}_{avg_valid_acc:.2f}.pt')\n                self.best_train_acc = avg_train_acc\n                self.best_val_acc = avg_valid_acc\n                self.wait = 0  # Reset patience counter\n            else:\n                self.wait += 1\n                print(f\"Validation loss did not improve. Patience counter: {self.wait}/{self.patience}\")\n                if self.wait >= self.patience:\n                    print(\"Early stopping condition met.\")\n                    self.early_stop = True\n\n            # Logging\n            print(f\"Epoch {i+1}/{epochs} | Train Loss: {avg_train_loss:.6f} | Train Acc: {avg_train_acc:.6f}\")\n            print(f\"Epoch {i+1}/{epochs} | Valid Loss: {avg_valid_loss:.6f} | Valid Acc: {avg_valid_acc:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:35.347896Z","iopub.execute_input":"2025-02-10T12:12:35.348201Z","iopub.status.idle":"2025-02-10T12:12:35.358814Z","shell.execute_reply.started":"2025-02-10T12:12:35.348177Z","shell.execute_reply":"2025-02-10T12:12:35.358047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# weighted loss","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nclass_labels = sorted(os.listdir(augmented_train_dir))  # Assuming folder names are class labels\nnum_classes = len(class_labels)\n\n# Count the number of samples per class\nclass_counts = {label: len(os.listdir(os.path.join(augmented_train_dir, label))) for label in class_labels}\n\n# Convert class labels to indices\nclass_to_idx = {label: idx for idx, label in enumerate(class_labels)}\ny_train = []\n\n# Collect all labels from the training dataset\nfor label, count in class_counts.items():\n    y_train.extend([class_to_idx[label]] * count)\n\n# Compute class weights using sklearn\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n\n# Convert to tensor for PyTorch loss function\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\nprint(class_weights_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:42.402632Z","iopub.execute_input":"2025-02-10T12:12:42.402968Z","iopub.status.idle":"2025-02-10T12:12:42.722367Z","shell.execute_reply.started":"2025-02-10T12:12:42.40294Z","shell.execute_reply":"2025-02-10T12:12:42.721576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:47.312911Z","iopub.execute_input":"2025-02-10T12:12:47.313236Z","iopub.status.idle":"2025-02-10T12:12:47.317732Z","shell.execute_reply.started":"2025-02-10T12:12:47.313211Z","shell.execute_reply":"2025-02-10T12:12:47.316803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = ModelTrainer(criterion,optimizer)\ntrainer.fit(model, train_loader, valid_loader, epochs = 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:12:49.152377Z","iopub.execute_input":"2025-02-10T12:12:49.152669Z","iopub.status.idle":"2025-02-10T15:07:14.605505Z","shell.execute_reply.started":"2025-02-10T12:12:49.152645Z","shell.execute_reply":"2025-02-10T15:07:14.604612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing ","metadata":{}},{"cell_type":"code","source":"def prediction(model_dir, data_loader, model):\n    model.load_state_dict(torch.load(model_dir))\n    model.eval()\n    loss = 0.0\n    acc = 0.0\n    for images,labels in tqdm(data_loader):\n            \n            # move the data to CPU\n            images = images.to(device) \n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs,labels)\n            \n            loss += loss.item()\n            acc += accuracy(outputs,labels)\n    \n    print(\"Test Loss : {:.6f} Test Acc : {:.6f}\".format(loss / len(data_loader), acc / len(data_loader)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:08:11.398118Z","iopub.execute_input":"2025-02-10T15:08:11.398447Z","iopub.status.idle":"2025-02-10T15:08:11.403471Z","shell.execute_reply.started":"2025-02-10T15:08:11.39842Z","shell.execute_reply":"2025-02-10T15:08:11.402485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/working/cnn_v10_res22k_0.94_0.97.pt\", test_loader, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:08:17.927316Z","iopub.execute_input":"2025-02-10T15:08:17.927628Z","iopub.status.idle":"2025-02-10T15:08:22.306915Z","shell.execute_reply.started":"2025-02-10T15:08:17.927605Z","shell.execute_reply":"2025-02-10T15:08:22.306032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/working/cnn_v10_res22k_0.95_0.98.pt\", test_loader, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:09:54.898554Z","iopub.execute_input":"2025-02-10T15:09:54.898854Z","iopub.status.idle":"2025-02-10T15:09:56.660297Z","shell.execute_reply.started":"2025-02-10T15:09:54.898815Z","shell.execute_reply":"2025-02-10T15:09:56.659423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction(\"/kaggle/working/cnn_v10_res22k_0.96_0.98.pt\", test_loader, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:10:25.989252Z","iopub.execute_input":"2025-02-10T15:10:25.989554Z","iopub.status.idle":"2025-02-10T15:10:27.739754Z","shell.execute_reply.started":"2025-02-10T15:10:25.989532Z","shell.execute_reply":"2025-02-10T15:10:27.738934Z"}},"outputs":[],"execution_count":null}]}